{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bb0d1fd-c8a7-4182-ada4-f5ec7663bf46",
   "metadata": {},
   "source": [
    "# Scraper Code for Analytics for Unstructured data project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e270cc7e-7b25-43e8-80c2-191aa045ac8b",
   "metadata": {},
   "source": [
    "**Team members:**\n",
    "\n",
    "1. Aditya Sindhavad\n",
    "2. Adithya Murali\n",
    "3. Advaith Shankar\n",
    "4. Manasa Maganti\n",
    "5. Shashank Rao\n",
    "6. Varsha Manju Jayakumar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe736963-4705-418a-b832-b3a451b06b6f",
   "metadata": {},
   "source": [
    "**Website Scraped** : https://www.fragrantica.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd7a9b6-6edf-4647-a3fe-da4355a788a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Load the Excel file that contains the URLs\n",
    "urls_df = pd.read_excel('url_listM.xlsx')  # Replace with the actual file name\n",
    "urls = urls_df['url']  # Assuming the column in the Excel file is labeled 'URLs'\n",
    "\n",
    "# Initialize a list to store the data\n",
    "scraped_data = []\n",
    "\n",
    "# Set up the WebDriver (e.g., Chrome)\n",
    "\n",
    "# Function to scroll the page by random units and wait a random amount of time\n",
    "def incremental_scroll(driver):\n",
    "    last_position = driver.execute_script(\"return window.pageYOffset;\")\n",
    "    while True:\n",
    "        # Scroll by a random number of pixels between 450 and 700\n",
    "        scroll_amount = random.randint(450, 700)\n",
    "        driver.execute_script(f\"window.scrollBy(0, {scroll_amount});\")\n",
    "        \n",
    "        # Wait for a random time between 1 and 4 seconds\n",
    "        time.sleep(random.uniform(1, 4))\n",
    "\n",
    "        new_position = driver.execute_script(\"return window.pageYOffset;\")\n",
    "        if new_position == last_position:\n",
    "            break  # Break if the scroll position hasn't changed (end of page reached)\n",
    "        last_position = new_position\n",
    "\n",
    "# Loop through each URL, scrape the content, and store it in the list\n",
    "for url in urls:\n",
    "    try:\n",
    "        # Open the website\n",
    "        driver = webdriver.Chrome()  # Make sure you have the Chrome WebDriver in your PATH\n",
    "        driver.get(url)\n",
    "\n",
    "        # Scroll to the end of the page\n",
    "        incremental_scroll(driver)\n",
    "\n",
    "        # Parse the page with BeautifulSoup after scrolling is complete\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "        # Append the URL and the soup content (as text) to the list\n",
    "        scraped_data.append({'URL': url, 'HTML Content': str(soup)})\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to scrape {url}: {e}\")\n",
    "\n",
    "    # Close the WebDriver after all URLs have been scraped\n",
    "    driver.quit()\n",
    "\n",
    "# Create a DataFrame from the scraped data\n",
    "scraped_df = pd.DataFrame(scraped_data)\n",
    "\n",
    "# Optionally, save the DataFrame to a CSV file\n",
    "#scraped_df.to_csv('scraped_websites_data.csv', index=False)\n",
    "\n",
    "# Display the DataFrame\n",
    "#print(scraped_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da87ce5e-e66b-40f4-9d97-1bc4d8dc980b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Assuming scraped_df already contains the scraped HTML content in 'HTML Content' column\n",
    "# with URLs in 'URL' column\n",
    "\n",
    "# Prepare a list to store the extracted review data\n",
    "review_data = []\n",
    "\n",
    "# Loop over each row in the scraped_df DataFrame\n",
    "for index, row in scraped_df.iterrows():\n",
    "    try:\n",
    "        # Get the HTML content and parse it with BeautifulSoup\n",
    "        soup = BeautifulSoup(row['HTML Content'], 'html.parser')\n",
    "\n",
    "        # Extract the page title\n",
    "        page_title = soup.title.get_text() if soup.title else 'No Title'\n",
    "\n",
    "        # Find all <div> elements with itemprop=\"reviewBody\"\n",
    "        review_divs = soup.find_all('div', itemprop='reviewBody')\n",
    "\n",
    "        # Loop through all the review divs and store the text content along with the page title\n",
    "        for div in review_divs:\n",
    "            review_content = div.get_text(strip=True)  # Extract the review content text\n",
    "            # Append the data as a dictionary to the list\n",
    "            review_data.append({\n",
    "                'URL': row['URL'],\n",
    "                'Title': page_title,\n",
    "                'Review': review_content\n",
    "            })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing URL {row['URL']}: {e}\")\n",
    "\n",
    "# Create a DataFrame from the review_data list\n",
    "reviews_df = pd.DataFrame(review_data)\n",
    "\n",
    "# Optionally, save the DataFrame to a CSV file\n",
    "reviews_df.to_csv('extracted_reviews_data4.csv', index=False)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(reviews_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
